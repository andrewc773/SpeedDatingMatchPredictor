{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523863cd-f1b7-4866-94b1-bacd78171b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Predicting a Decision in Speed Dating**\n",
    "## Andrew Chavarria\n",
    "\n",
    "<div align=\"center\"><img src=\"https://static01.nyt.com/images/2022/04/28/fashion/26SPEED-DATING1/26SPEED-DATING1-videoSixteenByNine3000.jpg\" width=500p></img></div>\n",
    "<div align=\"center\">Image from The New York Times</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c92b63-c0f0-4135-8138-ec32b6449c84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div align=\"center\"> Introduction </div>\n",
    "\n",
    "<p style=\"font-size: 17px\">Speed dating is a formalized matchmaking process where two groups of single participants \"date\" each other for a short period of time, in round-robin fashion. For each date, each participant must decide whether or not they would like to see the other party in the future. If both say \"Yes\",\n",
    "then there is a match. </p>\n",
    "<br>\n",
    "<p style=\"font-size: 17px\">Assembled by Columbia Business School professors, Ray Fisman and Sheena Iyengar, our <a href=\"https://www.kaggle.com/datasets/annavictoria/speed-dating-experiment\">dataset</a> holds data for speed dating events from 2002-2004. Each participant was required to enter information about themselves, such as age, school of undergraduate degree, and occupation. They were asked to rate (on a scale of 1-10 or 1-100) what they valued most in a partner. At the end of each date, they were also asked to rate the attributes of their partner on an individual level. Each date was between members of the oppposite sex. There were no entries from same-sex groups.</p>\n",
    "<br>\n",
    "<p style=\"font-size: 17px\">In this tutorial, our goal is to find trends in our data, especially comparing the two groups: men and women. By the end of the tutorial, we will have composed a Machine Learning model using a Decision Tree classification algorithm. We hope that by the end of this tutorial, you will have a better understanding of how and why one should handle null entries in a dataset, hypothesis testing, how to create an ML model with SKLearn, and how to evaluate the performance of such a model. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff662a-14d6-42ba-bf9b-e89f2bd1d9a6",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Data Munging</div>\n",
    "\n",
    "<p style=\"font-size: 17px\">First, let's import the libraries we need to handle and process our data.</p>\n",
    "\n",
    "<p style=\"font-size: 17px\">We'll be using <a href=\"https://pandas.pydata.org/\">Pandas</a>, <a href=\"https://scikit-learn.org/stable/\">Scikit-Learn</a>, <a href=\"https://numpy.org/\"> Numpy</a>, and <a href=\"https://scipy.org/\">Scipy</a>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da2190-827f-4efd-97c2-110744706a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_validate\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66c15b-1ee1-454e-9d21-018c0e622588",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">The next step is to acquire the dataset via the internet. You can download the zip file <a href=\"https://www.kaggle.com/datasets/annavictoria/speed-dating-experiment/download?datasetVersionNumber=1\">here</a>.\n",
    "<br><br>\n",
    "<p style=\"font-size: 17px\">Now, convert the CSV File into a Pandas DataFrame. Initially, we run into an issue with encoding, because our CSV file is not UTF-8 encoded, so we change the encoding as a parameter passed into read_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca21a8c-803e-4561-853d-f3f5ba6bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Speed Dating Data.csv\", encoding='iso-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b066be-875b-40db-a41a-e0e3678278a8",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Above, you can see that there are 195 columns. 195 potential features in our ML model. Wow, that's a lot of features! It is also easy to see\n",
    "from the above entries that a lot of the columns have been left empty. That is problematic, because we need real data in order to form our prediction model.\n",
    "\n",
    "### Data Removal\n",
    "<p style=\"font-size: 17px\">We will be removing all columns that involve data entries after the participant has already decided whether or not they are a match with their particpant, since this data is irrelevant for the question we want to answer. We want to be able to predict if a person will match with someone solely from the initial speed date they had, and the information about themselves that they filled out prior (such as gender, age, field of study, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362dfcd-b250-4330-bc7b-a0f9bcfbef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove every column after \"amb3_s\", since the entries after that are filled out after the speed date event.\n",
    "data = data.iloc[:, :data.columns.get_loc('satis_2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2663eb4-9c39-438e-8650-31e508b32349",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Some columns require altering, particularly for missing data in the columns. There are several ways to handle missing data, and it is context-dependent. For the mn_sat column, which marks the median SAT score from the participant's school for undergrad, we will look up the mean score back in 2002 to fill the column. This column is important because it serves as a proxy for overall intelligence.\n",
    "\n",
    "<p style=\"font-size: 17px\">I manually looked up average SAT score from the school of undergrad that the participants have gone to, and assigned the SAT value in a dictionary. From here, I will fill out the mn_sat score for any participants whose mn_sat was left empty, but have listed their school of undergraduate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01195ea9-46de-47ed-b12d-f3dee9b77bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_map = {'Washington U. in St. Louis': 1469, 'university of wisconsin/la crosse': 1174 ,'HOWARD UNIVERSITY': 1213, 'Cambridge University': 1480, 'Illinois': 1350, \n",
    "       'Penn State University': 1265, 'George Washington University': 1370, 'University of Wisconsin-Madison': 1390, 'University of Illinois/Champaign': 1350, 'GW': 1370,\n",
    "       'University of Chicago': 1520, 'UC Irvine': 1310, 'Cal State Univ.,Long Beach': 1145, 'Washington University in St. Louis': 1469, 'Tufts University': 1465, \n",
    "       'University of Rochester': 1420,  'Rice University': 1505, 'washington university in st louis': 1470, 'Tufts': 1465, 'tufts': 1465, \n",
    "       'University of the Philippines': 1026, 'UM': 1355, 'Wake Forest': 1390, 'University of Massachusetts-Amherst': 1290, 'Case Western Reserve University': 1435, \n",
    "       'Vanderbilt University': 1505, 'University of Kansas': 1240, 'California State University Los Angeles': 985, 'COOPER UNION': 1413, 'Texas State University': 1095,\n",
    "       'Cooper Union, Bard college, and SUNY Purchase': 1304, 'University of Genova': 1127, 'Oxford University': 1550, 'Oxford': 1550}\n",
    "\n",
    "# Fill in all missing SAT scores for schools in which we were able to locate SAT score averages\n",
    "for k in sat_map.keys():\n",
    "    data.loc[ data['undergra'] == k, 'mn_sat'] = sat_map[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e180c-2a32-4ef7-967b-08e34c0111e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[pd.isna(data.mn_sat) == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34f2ce-216b-4552-933f-c42738443a98",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">There are still plenty of missing values for mn_sat, so we will replace these with the median SAT score so far of the participants who have on assigned to them. There is an issue, however. Most of our mn_sat scores are entered as strings, so we will have to convert the entire column to floating point values in order to calculate the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4607947-f233-4892-b3ff-72a4469ba5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN values with an empty string, set all values in SAT score to strings, then convert all strings to floats for SAT\n",
    "#so we can calculate median score.\n",
    "data[\"mn_sat\"] = data['mn_sat'].replace(np.nan, '')\n",
    "data[\"mn_sat\"]= data[\"mn_sat\"].astype(str)\n",
    "data[\"mn_sat\"] = pd.to_numeric(data[\"mn_sat\"].replace(',','', regex=True), downcast=\"float\")\n",
    "\n",
    "# Replace NaN values with the SAT median score of the entire population\n",
    "data[\"mn_sat\"] = data['mn_sat'].replace(np.nan, data[\"mn_sat\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1526a1-7553-44b0-ab9f-32c544eb07f2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Another column we will fill the missing entries of is salary. For this column, we will just calculate the mean salary of all participants and replace all missing salary entries with this mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad066fb-a886-4ff9-8c1b-e8a902be3e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"income\"] = data['income'].replace(np.nan, '')\n",
    "data[\"income\"]= data[\"income\"].astype(str)\n",
    "data[\"income\"] = pd.to_numeric(data[\"income\"].replace(',','', regex=True), downcast=\"float\")\n",
    "\n",
    "data[\"income\"] = data['income'].replace(np.nan, data['income'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306039a-44dc-4be2-b290-fc508689d993",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Other columns we will want to remove are the IDs of the participants, IDs of their speed date partner, zipcode, and some of the categorical data that have already been encoded in numbers for us, such as career and field of study in undergrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f14e6-2078-451f-8e71-6e6eaabb1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['iid', 'id', 'idg', 'position', 'positin1', 'partner', 'pid', 'field', 'undergra', 'from', 'zipcode', 'career', 'tuition', 'expnum'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d4e53-e848-4965-b6d9-62e15a8d2c33",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Now, let's see where we stand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542d1ea-6628-4c02-958a-d871ad3de774",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,['attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d37e0-2223-4532-8e6b-2fc5ae52db6f",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">There are still too many NaN entries for our dataset. We will remove the columns who just have too many null entries, at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb31ee9-5e0e-48e1-a0cc-7a01ece784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:, :data.columns.get_loc('attr1_s')]\n",
    "data = data.drop(['attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17470cb3-c9e5-4dc0-a050-2f4418727cbe",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">We will also delete all entries that have missing values in the participant's impression of their date. The ratings of the participant's date are crucial to our prediction, and it wouldn't make sense to replace data with the mean or median of a rating if every person's impression can vary greatly (you may really like a particular person, or really not like them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685ee82-3cc1-4752-8de7-3de22611eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any row with a missing entry in any of these columns\n",
    "data = data[pd.isna(data.attr) == False]\n",
    "data = data[pd.isna(data.sinc) == False]\n",
    "data = data[pd.isna(data.intel) == False]\n",
    "data = data[pd.isna(data.fun) == False]\n",
    "data = data[pd.isna(data.amb) == False]\n",
    "data = data[pd.isna(data.shar) == False]\n",
    "data = data[pd.isna(data.like) == False]\n",
    "data = data[pd.isna(data.prob) == False]\n",
    "data = data[pd.isna(data.met) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eaf685-cbb9-4bac-9b56-e086f80ced79",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Another issue we run into: Out of the 21 waves of dating, waves 6-9 use a rating scale of 1-10 for attributes they are rating, as opposed to an 100-point scale, which is used by all of the other waves. So, we will take each rating from wave 6-9 and multiply it by 10, so it fits an 100-point scale. This is important for replacing nan attribute values with the median/mean of said attribute.\n",
    "<br><br>\n",
    "<p style=\"font-size: 17px\">It appears that most attributes have been converted to an 100-point scale for waves 6-9, except attr4_1 to shar4_1, so we will adjust those accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab2ed3-5214-491e-abfc-30d5a9d9e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data mask to get all entries from waves 6-9\n",
    "mask = ((data['wave'] >=6) & (data['wave'] <= 9))\n",
    "waves_6_9 = data[mask]\n",
    "# iterate through each of these rows, and multiply their attribute rating by 10 to fit an 100-point scale\n",
    "for i, row in waves_6_9.iterrows():\n",
    "    data.at[i, 'attr4_1'] = waves_6_9['attr4_1'][i]*10\n",
    "    data.at[i, 'sinc4_1'] = waves_6_9['sinc4_1'][i]*10\n",
    "    data.at[i, 'intel4_1'] = waves_6_9['intel4_1'][i]*10\n",
    "    data.at[i, 'fun4_1'] = waves_6_9['fun4_1'][i]*10\n",
    "    data.at[i, 'amb4_1'] = waves_6_9['amb4_1'][i]*10\n",
    "    data.at[i, 'shar4_1'] = waves_6_9['shar4_1'][i]*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1e0e4-e2b7-4908-bb36-cb1c1a846db7",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Some empty entries cannot be replaced by the mean or median of the column. This is true if the variable is a categorical, such as the race of speed date partner or career choice. So, we will go ahead and delete any empty entries for those attributes.\n",
    "\n",
    "<br>\n",
    "<p style=\"font-size: 17px\">Then, we will replace any empty values with the median for the rest of the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7649018-0532-4e7a-9c7b-b5bd84b54e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where entries cannot be replaced by the median. \n",
    "not_replaceable = ['race_o', 'met_o', 'field_cd', 'career_c']\n",
    "\n",
    "for i in not_replaceable:\n",
    "    data = data[pd.isna(data[i]) == False]\n",
    "# replace the rest of the columns with their median, if they contain a NaN value:\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].replace(np.nan, data[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590dbb73-d0a5-412d-a71a-b668332b16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(col+\" empty entry count: \"+ str(len(data[pd.isna(data[col])] == True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be82ae-531e-4373-a9ad-a0229fa8ce53",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">There are no more empty entries in our dataset! Phew, that was a lot of work. Now we can begin to explore the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87d1e4-b068-44d4-862a-c07db5bb27d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div align=\"center\"> Exploring the Data </div>\n",
    "\n",
    "<p style=\"font-size: 17px\">To begin our exploratory analysis, we will break the data into two separate groups: Male and Female. We could have chosen other ways to separate groups, such as age or race. But it makes intuitive sense to classify based on gender, since the two different groups being paired in the speed dating experiment were male and female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9715ef0-ca16-408b-a0ba-9453b490a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataframes from our data, distinguishing between male and female.\n",
    "female_df = data[data['gender'] == 0]\n",
    "male_df = data[data['gender'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7f90c-a325-42cd-8a0f-3ab2c5711ba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"font-size: 17px\">Now let's answer some questions with our given dataframes. For example, are women and men equally as selective when deciding whether or not they want to see someone again after a date?\n",
    "\n",
    "<p style=\"font-size: 17px\">To begin to answer this question, we will want to know what information we need. Our number of males and females are roughly equivalent (slightly more males), so we may be able to answer our own question with a simple data visualization, if the difference is stark enough. We will plot this on a bar graph using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a600d-389e-4ede-87c0-89293cb152e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the decisions of males and females for \"Yes\" and \"No\" for the dec attribute, and put into an array\n",
    "male_dec = [male_df['dec'].value_counts()[0], male_df['dec'].value_counts()[1]]\n",
    "female_dec = [female_df['dec'].value_counts()[0], female_df['dec'].value_counts()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d32852-f710-4dca-9d9e-821977c0feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X-axis array\n",
    "X = ['No', 'Yes']\n",
    "\n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "#create bar for men and women\n",
    "plt.bar(X_axis - 0.2, male_dec, 0.4, label = 'Men')\n",
    "plt.bar(X_axis + 0.2, female_dec, 0.4, label = 'Women')\n",
    "\n",
    "# Set the parameters for the bar graph, such as labeling and title, then display.\n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Decision to match with date\")\n",
    "plt.ylabel(\"Number of Decisions\")\n",
    "plt.title(\"Decision of men and women in Speed Dating\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c5315-60b6-4ca0-885d-d10ba56dcf16",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"font-size: 17px\">Interesting. Despite a slightly larger amount of men, more women than men decide \"No\", and are also far less likely to say \"yes\". Men are nearly 50/50 in their decision split. It is clear that women were a bit more choosy than men, on average, in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0cc8d-a94b-4a67-93f5-772c3923481a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a Decision Tree Model\n",
    "\n",
    "<p style=\"font-size: 17px\">Since we are also making a classification  model, we will want to save the data and targets for the respective dataframes, for later.\n",
    "<br>\n",
    "<p style=\"font-size: 17px\">We will get the input of decisions from the participants in the form of an array, and then remove the targets from the dataset, since we are using our ML model to decide whether the participant will decide \"yes\" or \"no\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6ea00-0934-4c9f-9f19-fe7aaa7b74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the respective data and targets according to gender:\n",
    "female_target = np.array(female_df.dec)\n",
    "female_data = female_df.drop('dec', axis = 1)\n",
    "female_X, female_y = female_data.to_numpy(), female_target\n",
    "\n",
    "\n",
    "male_target = np.array(male_df.dec)\n",
    "male_data = male_df.drop('dec', axis = 1)\n",
    "male_X, male_y = male_data.to_numpy(), male_target\n",
    "\n",
    "# Data and target for whole dataframe\n",
    "target = np.array(data.dec)\n",
    "data = data.drop('dec', axis = 1)\n",
    "X, y = data.to_numpy(), target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27da70-8862-421b-aa73-3b164d0ce84a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fitting and testing our model\n",
    "<p style=\"font-size: 17px\">For each of the data and its targets, we can fit a Decision Tree, and then run 10-fold cross validation (with SKLearn) to estimate the skill of our new model.\n",
    "<br>\n",
    "\n",
    "<p style=\"font-size: 17px\">Our performance metrics chosen will be accuracy, recall, and precision of our test data, in order to give an all-around impression of just how well it performs. We will also compute the mean squared error, to report Cross Validation error for our algorithm. The lower the MSE, the better.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style=\"font-size: 17px\">Each cross-validation produces each of these scores, so we will take the mean of all 10.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14d85a-b782-43fd-8ce2-8625cde7365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 10, splitter = 'best', criterion = \"entropy\")\n",
    "female_clf = clf.fit(female_X, female_y)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "scoring = {'mse':'neg_mean_squared_error', 'acc': 'accuracy', 'prec_macro': 'precision_macro','rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores = cross_validate(clf, female_X, female_y, scoring=scoring, cv=10, return_train_score=True)\n",
    "\n",
    "print(\"Female Test data performance: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['test_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['test_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for test data\" % (np.mean(scores['test_mse'])))\n",
    "print(\"\\nFemale Training data performance: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['train_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['train_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for training data\" % (np.mean(scores['train_mse'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5875d-6a09-4631-8003-170a15bcddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 10, splitter = 'best', criterion = \"entropy\")\n",
    "male_clf = clf.fit(male_X, male_y)\n",
    "\n",
    "scoring = {'mse':'neg_mean_squared_error', 'acc': 'accuracy', 'prec_macro': 'precision_macro','rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores = cross_validate(clf, male_X, male_y, scoring=scoring, cv=10, return_train_score=True)\n",
    "\n",
    "print(\"Male Test data performance: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['test_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['test_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for test data\" % (np.mean(scores['test_mse'])))\n",
    "print(\"\\nMale Training data performance: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['train_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['train_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for training data\" % (np.mean(scores['train_mse'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0975fd-4d27-4047-a082-b679a5a2485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 10, splitter = 'best', criterion = \"entropy\")\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "scoring = {'mse':'neg_mean_squared_error', 'acc': 'accuracy', 'prec_macro': 'precision_macro','rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores = cross_validate(clf, X, y, scoring=scoring, cv=10, return_train_score=True)\n",
    "\n",
    "print(\"Males and Females Combined Test data performance: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['test_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['test_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for test data\" % (np.mean(scores['test_mse'])))\n",
    "print(\"\\nMales and Females Combined Training data performance: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['train_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['train_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for training data\" % (np.mean(scores['train_mse'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a0833-64d6-46e6-aefd-336094bfa2e7",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Our Decision Tree models for males and females perform worse at predicting decisions after a date, as opposed to taking the entire dataset as a whole. Why is this? It could be minor overfitting, since our training data performs about 10% better on accuracy and recall when compared to the training data for males and females. Remember, we are cutting our training data in half when we split dateframes into male and female.\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"font-size: 17px\">This makes sense, because when we use the dataframe combining both males and females, we are creating more training data for the model, which improves the test data performance and results in less overfitting in the Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af77b83-a038-4be6-9f4c-4ddaac47b263",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "<p style=\"font-size: 17px\">SKLearn contains several hyperparameters for the DecisionTreeClassifier to create a Decision Tree, such as max_features, min_weight_fraction_leaf, min_samples_split, and many more. To keep our model relatively simple for beginners, we will tune the max_depth (how deep our tree recurses down), to show differences in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50137cd-849f-425e-b5fa-991d4c98d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 20, splitter = 'best', criterion = \"entropy\")\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "scoring = {'mse':'neg_mean_squared_error', 'acc': 'accuracy', 'prec_macro': 'precision_macro','rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores = cross_validate(clf, X, y, scoring=scoring, cv=10, return_train_score=True)\n",
    "\n",
    "print(\"Males and Females Combined Test data performance with max_depth of 20: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['test_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['test_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for test data\" % (np.mean(scores['test_mse'])))\n",
    "print(\"\\nMales and Females Combined Training data performance with max_depth of 20: \")\n",
    "print(\"10-fold Cross Validation results for Decision Tree on the entire dataset:\")\n",
    "print(\"%0.2f accuracy for test data\" % (np.mean(scores['train_acc'])))\n",
    "print(\"%0.2f recall for test data\" % (np.mean(scores['train_rec_macro'])))\n",
    "print(\"%0.2f precision for test data\" % (np.mean(scores['train_prec_macro'])))\n",
    "print(\"%0.2f mean_squared error for training data\" % (np.mean(scores['train_mse'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0618-dc3b-4a93-aa16-248eb58beca0",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Take for example above, where we set our Decisiom Tree max_depth to 20 for the entire datset. This improved our training set performance to 100% for several metrics, including a 0.0 MSE, but our test set performance suffered, suggesting overfitting. With hyperparameters, it is important to tune them to reach acceptable rates of performance. Unfortunately, tuning hyperparameters is not an exact science and can require trial-and-error. I did several trial-and-error rounds to pick a good max_depth prior to this tutorial, and a value of 10 seemed to do well for all sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809246e-7522-4144-9bd9-92a3f909e286",
   "metadata": {},
   "source": [
    "### Making sense of Feature Importance in a Decision Tree model\n",
    "<p style=\"font-size: 17px\">We know what a Decision Tree decides to split nodes at is relatively simple; it takes a greedy approach by splitting at the feature with the highest \"purity\", or least entropy among the other features to split at in order to make a decision. This gives use the highest information gain.</p>\n",
    "\n",
    "\n",
    "**Algorithm**:\n",
    "<br>\n",
    "    while(features_remaining!=null):<br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;next_split = least_entropy(features_remaining)<br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;featurs_remaining.remove(next_split)<br><br>\n",
    "\n",
    "**Entropy**: \n",
    "<br>\n",
    "<img src = \"https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-004399bf736b5463087bdd15d256e30e_l3.svg\" width=300p></img>\n",
    "<br>*Image from baeldung.com*\n",
    "<br>\n",
    "\n",
    "<p style=\"font-size: 17px\">Scikit learn's Decision Tree classifier has an attribute, *feature_importances_*, which returns an array of each feature's importance for the model, normalized. Let's take the top five important features for men and women when making a decision, and see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b13b49-eccf-4212-a16c-c17c7ed0f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Tree model feature_importances_ array\n",
    "male_feature_arr = male_clf.feature_importances_\n",
    "# Map the name of the feature to its importance within the feature_importances_ array\n",
    "male_dict = dict()\n",
    "index = 0\n",
    "for col in male_data.columns:\n",
    "    male_dict[col] = male_feature_arr[index]\n",
    "    index+=1\n",
    "# Sort the importance in ascending order\n",
    "male_arr = sorted(male_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "# Repeat the same process for females:\n",
    "female_feature_arr = female_clf.feature_importances_\n",
    "female_dict = dict()\n",
    "index = 0\n",
    "for col in female_data.columns:\n",
    "    female_dict[col] = female_feature_arr[index]\n",
    "    index+=1\n",
    "female_arr = sorted(female_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "# Extract the top 5 most important attributes for males and females when making a decision\n",
    "male_attributes_top5 = [i[0] for i in male_arr[-5:]]\n",
    "female_attributes_top5 = [i[0] for i in female_arr[-5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c9aa2-c4b5-4ef6-9c64-bb0e4d8fc525",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Now, we can create a barplot comparing importance of attribute for each gender. In our case, the top five attributes were all the same, though the importance value was different between genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e462f7-7eb0-437d-a8d2-1e726ead52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10, 3.50]\n",
    "X_axis = np.arange(len(male_attributes_top5))\n",
    "plt.bar(X_axis- 0.2 , [i[1] for i in male_arr[-5:]], 0.4, label = 'Men')\n",
    "X_axis = np.arange(len(female_attributes_top5))\n",
    "plt.bar(X_axis+ 0.2 , [i[1] for i in female_arr[-5:]], 0.4, label = 'Women')\n",
    "plt.xticks(X_axis, male_attributes_top5)\n",
    "plt.xlabel(\"Attribute\")\n",
    "plt.ylabel(\"Coefficient value of feature_importance_\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55283839-04d2-4bc1-8312-c92b82eefed4",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Attractiveness of the partner appears to be a better predictor of a decision for men than it does for women. The same is true for how much the person \"likes\" their date. Interestingly enough, the best predictors for a decision are based on the decision of the person's date, and whether or not their was a match. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb18344-18bf-4f49-8e8d-93170d626a34",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Hypothesis Testing</div>\n",
    "\n",
    "<p style=\"font-size: 17px\">We will create a new column in our dataset, difference_of_age, which takes the difference in age between the participant and their date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144ed8a-3c37-4ec0-af5f-33515a4315e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column, 'difference_of_age', and initialize with value 0 for all rows.\n",
    "data['difference_of_age'] = 0\n",
    "# For each row, calculate the difference in age between the participant and their date.\n",
    "for i, row in data.iterrows():\n",
    "    data.at[i, 'difference_of_age'] = data['age_o'][i] - data['age'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b23c8-d41b-4b31-a7f3-260e6c159c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean difference in age between any date: \"+str(data['difference_of_age'].mean()))\n",
    "print(\"Mean difference in age between matched dates: \"+str(data[data['match'] == 1]['difference_of_age'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53197180-0c90-474b-a2f3-57ef6d479470",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">The mean difference in age is larger between matched dates, but is it statistically significant? Our null hypothesis would be that there is no statistical significance to the difference in age; it is just a product of sampling or experimental error.</p>\n",
    "\n",
    "### Checking normal distribution\n",
    "\n",
    "<p style=\"font-size: 17px\">Now we need to check if our data is normally distributed to see if we can perform a T-Test to compare two different groups here. The two differing groups being the normal populatiom, and those who matched with their date. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc2ce0-ad30-468f-a55e-f8410701bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#assign histogram bar to all dates\n",
    "ax.hist(data['difference_of_age'], 10, label='All dates', stacked=True)\n",
    "#assign histogram bar to matched dates\n",
    "ax.hist(data[data['match'] == 1]['difference_of_age'], 10, label='Matched dates', stacked=True)\n",
    "# Make the chart look pretty\n",
    "ax.legend()\n",
    "ax.set_title(\"Distribution of age difference among man and woman on speed date\")\n",
    "ax.set_xlabel(\"Age difference in years\")\n",
    "ax.set_ylabel(\"Number of dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe185073-b1c1-481e-8125-5746105db2a4",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\">Both of the groups seem to have a relatively normal distribution for difference in age. We can perform the Two-Sample T-Test, using Scipy's library, to compute the P-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bceb7b-62f7-4546-9419-07dd1a66abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-test, using Scipy, on the different groups\n",
    "stats.ttest_ind(data['difference_of_age'], data[data['match'] == 1]['difference_of_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b465bbe-272e-438e-8a24-6f48e4094ab6",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px\"> The P-Value is not &lt;= 0.05, so the difference in age is not statistically significant between dates that match, compared to all dates. Therefore, the null hypothesis holds. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144165ea-c45a-47a5-b7cd-f0963027ba80",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align=\"center\"><img src=\"https://i0.wp.com/post.psychcentral.com/wp-content/uploads/sites/4/2022/01/couple-silhouette-sunset-holding-hands-walking-grass-1296x728-header-1024x575.jpg?w=1155&h=1528\" width=500p></div>\n",
    "<div align=\"center\">Copyright: Westend61 / Ezequiel Gim√©nez</div>\n",
    "\n",
    "## <div align=\"center\">Conclusion</div>\n",
    "\n",
    "<p style=\"font-size: 17px\">Dating and romance can seem complicated and confusing. Using data science, my hope was that this exploration would make the dating world easier to understand. Obviously, love is more complex than numbers, and it is difficult to capture what makes two people click, when everyone is so different. That said, our ML model was able to create a prediction that was better than 50/50 guessing when predicting if someone would like to see another person again or not. \n",
    "\n",
    "<p style=\"font-size: 17px\">So what should someone take away from our findings? It is noting that whether a match existed, or if the other person said, \"yes\", too, were hands-down the best predictors of a \"Yes\" or a \"No\". It perhaps suggests that people are able to read each other and have an intutive sense of whether or not they get along with someone else. If you were ever unsure of your relationship standing with someone else, or had a bad date experience with them, then chances are, they may felt the same way. And the same goes for positive experiences. Our findings do allude to differences in dating preferences between men and women too. There are several other studies exploring the differences, and our findings do seem to fit those of the other studies, such as larger emphasis placed on phyiscial attractiveness for men as opposed to women.\n",
    "\n",
    "<p style=\"font-size: 17px\">Our exploration has much room for improvement. There are several different factors that are still left to explore, such as relationships between matches and differences in intellgience or income. As for rigorous statistical analysis, one should keep in mind to control for other variables to make their findings as precise and accurate as possible. Machine learning can be a powerful tool for reading social dynamics, but newer, fuller, datsets are hopefully available in the near future so we can try and make sense of the world through the patterns and relationships they reveal to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3d963-6920-4cf0-95b3-4e6a16a18527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
